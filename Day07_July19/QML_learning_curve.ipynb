{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Potentials Exercises July 19 \n",
    "## QML package and QM7 learning curves\n",
    "\n",
    "In this exercise, we will work with QM7 again, but this time we will focus on learning curves. Learning curves are plots used to show a model's performance as the training set size increases. They give us information about the generalization error and can also predict how large a training set we need to get the desired accuracy. To speed up and simplify the work, we will use the QML package for this task. \n",
    "\n",
    "QML is a Python2/3-compatible toolkit for representation learning of properties of molecules and solids. The goal is to provide usable and efficient implementations of concepts such as representations and kernels. QML supplies the building blocks to carry out efficient and accurate machine learning on chemical compounds.\n",
    "\n",
    "Documentation: https://www.qmlcode.org/index.html#<br /> \n",
    "GitHub repository: https://github.com/qmlcode/qml<br /> \n",
    "GPU version: https://github.com/nickjbrowning/QMLightning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qml\n",
    "import time\n",
    "from qml.kernels import gaussian_kernel\n",
    "from qml.math import cho_solve\n",
    "from qml.representations import get_slatm_mbtypes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect QML Compound\n",
    "The QML package uses `Compound` objects that store all information from xyz files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol = qml.Compound(xyz=\"qm7_files/qm7/0001.xyz\")\n",
    "print(mol.coordinates)\n",
    "print(mol.atomtypes)\n",
    "print(mol.nuclear_charges)\n",
    "print(mol.name)\n",
    "print(mol.unit_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load QM7 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load molecules into Compounds\n",
    "xyzs = sorted(glob(\"qm7_files/qm7/*.xyz\"))\n",
    "mols = [qml.Compound(x) for x in xyzs]\n",
    "# Load energies\n",
    "energies = pd.read_csv('qm7_files/hof_qm7.txt', header=None, sep='\\s', engine='python')\n",
    "energies.columns = ['filename', 'PBE0', 'DFTB']\n",
    "y = energies.PBE0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Coulomb matrix\n",
    "We will start with the Coulomb matrix as a molecular descriptor due to its simplicity. We will use KRR (with rbf kernel) but this time deploying the QML package. Let us generate Coulomb matrices for the whole dataset. It could take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Coulomb matrix for all molecules. \n",
    "cm = np.array([np.array(qml.representations.generate_coulomb_matrix(\n",
    "                                    mol.nuclear_charges,                               \n",
    "                                    mol.coordinates,\n",
    "                                    size=23,\n",
    "                                    sorting=\"row-norm\"))\n",
    "       for mol in mols],dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Use the optimized hyperparameters that you have found in the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "####YOUR TURN: fill in your optimal alpha and gamma for the 'rbf' kernel from the previous exercise\n",
    "alpha = \n",
    "gamma = \n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve is the dependence of the test error like mean absolute error (MAE) on the size of the training set. All molecules that are not included in the training set belong to the test set. To plot this dependence, we need to know the MAE for different train set sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired train set sizes\n",
    "train_ratio = [0.004506408, 0.014082524, 0.044500775, 0.14082524, 0.445289396]\n",
    "total_mol = y.shape[0]\n",
    "train_size = [x*total_mol for x in train_ratio]\n",
    "test_error = []\n",
    "\n",
    "for ratio in train_ratio:\n",
    "    # Split data to train and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(cm, y, train_size = ratio, test_size = 1 - ratio)\n",
    "\n",
    "    # Calculate kernel matrix\n",
    "    K = gaussian_kernel(X_train, X_train, 1/np.sqrt(2*gamma))\n",
    "\n",
    "    # Add a small lambda to the diagonal of the kernel matrix\n",
    "    K[np.diag_indices_from(K)] += alpha\n",
    "\n",
    "    # Use the built-in Cholesky-decomposition to find c\n",
    "    c = cho_solve(K, Y_train)\n",
    "\n",
    "    # Calculate a kernel matrix between test and training data, using the same gamma\n",
    "    K_test = gaussian_kernel(X_test, X_train, 1/np.sqrt(2*gamma))\n",
    "\n",
    "    # Make the predictions\n",
    "    Y_predicted = np.dot(K_test, c)\n",
    "    \n",
    "    # Calculate mean absolute-error (MAE)\n",
    "    mae = np.mean(np.abs(Y_predicted - Y_test))\n",
    "    \n",
    "    # Calculate training size\n",
    "    size = ratio * total_mol\n",
    "    \n",
    "    test_error.append(mae)\n",
    "    print(\"Training size: %i\\t MAE: %.2f kcal/mol\" % (size, mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(train_size, test_error, color=\"red\", ls=\"-\", marker =\"o\", label=r'Coulomb matrix')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('MAE (kcal/mol)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should observe linearity, although this may not always be the case. Repeat the training again and observe how the errors and plot change slightly. This comes from splitting the molecules into test and train datasets differently each time. Also, not all molecules carry identical information. Therefore we should not take just one model, but always the average of several with different train/test distributions. Implement an average over several models.\n",
    "\n",
    "**Task 2:**Â Fill in code such that you will get an average of 10 ensembles for each training ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio_cm = [0.004506408, 0.014082524, 0.044500775, 0.14082524, 0.445289396]\n",
    "train_size_cm = [x*y.shape[0] for x in train_ratio_cm]\n",
    "test_error_cm = []\n",
    "ensemble = 10\n",
    "\n",
    "for ratio in train_ratio:\n",
    "    sum_mae = 0\n",
    "    for x in range(ensemble):\n",
    "        ####################################\n",
    "        ####YOUR TURN: Train KRR model and find MAE. Store MAE in mae variable.\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################################\n",
    "        sum_mae = sum_mae + mae\n",
    "    test_error_cm.append(sum_mae/ensemble)\n",
    "    size = ratio * total_mol\n",
    "    print(\"Training size: %i\\t Averaged MAE: %.2f kcal/mol\" % (size, sum_mae/ensemble))\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(train_size_cm, test_error_cm, color=\"red\", ls=\"-\", marker =\"o\", label=r'Coulomb matrix')\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('MAE (kcal/mol)')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FCHL19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Coulomb matrix, we have demonstrated how a learning curve is constructed. However, due to its simplicity, this descriptor does not achieve excellent results. We now move on to the FCHL19 descriptors, which are widely used due to their great performance.\n",
    "\n",
    "Briefly described, the FCHL19 representation is a vector that encodes the atomic environment of an atom in a chemical compound. It consists of a two-body term that encodes radial distributions between the central atoms and neighboring atoms of a given element type. Additionally, the representation contains a three-body term that encodes the mean distances and angles between the atom and neighboring pairs of atoms of given element types. This representation does not include an explicit one-body term. Please refer to the paper for more details: https://aip.scitation.org/doi/10.1063/1.5126701\n",
    "\n",
    "The higher accuracy is compensated by an increase in computational complexity. Therefore, in this part of the exercise, we will not average models and will train the models only up to 1000 molecules in the training set.\n",
    "\n",
    "Your task is to plot a learning curve for KRR with FCHL19 descriptors for training set ratios of 0.004506408, 0.014082524, 0.044500775, and 0.14082524 without averaging. Below we provide an example of how to train the FCHL19 model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate FCHL19 representation for all molecules.\n",
    "fchl = np.array([qml.fchl.generate_representation(mol.coordinates,\n",
    "                                                  mol.nuclear_charges)\n",
    "        for mol in mols],dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** For the FCHL19 representation, get MAEs for multiple training ratios (0.004506408, 0.014082524, 0.044500775, and 0.14082524) without averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio_fchl = [0.004506408, 0.014082524, 0.044500775, 0.14082524]\n",
    "train_size_fchl = [x*y.shape[0] for x in train_ratio_fchl]\n",
    "test_error_fchl = []\n",
    "\n",
    "####################################\n",
    "####YOUR TURN: Split data to train and test sets\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "\n",
    "# Hyperpamateres. You do not need to tune.\n",
    "alpha = 1e-7\n",
    "sigmas = [1.0]\n",
    "\n",
    "# Calculate kernel matrix for train set\n",
    "K = qml.fchl.get_local_kernels(X_train, X_train, sigmas, cut_distance=10.0)[0]\n",
    "\n",
    "# Add a small lambda to the diagonal of the kernel matrix\n",
    "K[np.diag_indices_from(K)] += alpha\n",
    "\n",
    "# Use the built-in Cholesky-decomposition to find c\n",
    "c = cho_solve(K,Y_train)\n",
    "\n",
    "# Calculate a kernel matrix between test and training data\n",
    "K_test = qml.fchl.get_local_kernels(X_test, X_train, sigmas, cut_distance=10.0)[0]\n",
    "\n",
    "\n",
    "####################################\n",
    "####YOUR TURN: Make the predictions, calculate and print the mean absolute-error (MAE)\n",
    "\n",
    "\n",
    "\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Plot how test_error_cm depends on train_size_cm together with how test_error_fchl depends on train_size_fchl. It allows us to compare the Coulomb matrix with FCHL19 representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "####YOUR TURN: Plot learning curves for the CM descriptor together with FCHL19 for comparison\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Using time.time(), compare the computation complexity of the Coulomb matrix and FCHL19. Also, determine whether the training takes longer than the prediction or vice versa. Empirically find the O(N) scaling for both training and prediction, where N is the dataset size. \n",
    "\n",
    "Take inspiration from the previous exercise located in scikit_KRR_SVR.ipynb how to use time.time()."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
