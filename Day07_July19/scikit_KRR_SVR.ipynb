{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Potentials Exercises July 19 \n",
    "## Kernel Ridge Regression and Support Vector Regression\n",
    "\n",
    "\n",
    "### Adapted from course: 2021W 270080-1 Machine learning for molecules and materials - Philipp Marquetand\n",
    "\n",
    "In this exercise, we will learn how to train Kernel ridge regression (KRR) and Support vector machine (SVM) models. We will use the QM7 dataset. It contains more than 7000 different molecules with a maximum number of 23 atoms per molecule. We will try different types of kernels, vary their hyperparameters and use the Coulomb matrix as our chemical environment descriptor. Finally, we compare KRR and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import and inspect QM7 data\n",
    "We will work with only 3500 molecules of the QM7 dataset due to time restrictions. The `qm7_files` directory consists of the `hof_qm7.txt` file and a `qm7` subdirectory with 7172 xyz-files. The `hof_qm7.txt` file contains the path to a molecule's xyz-file and its DFT/PBE0 and DFTB3 energy (heat of formation, hof). Each xyz-file represents one molecule. It contains information about the coordinates of the atoms, their chemical identity and their charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QM7 molecules\n",
    "#We will include 3500 data points\n",
    "n_molecules = 3500\n",
    "data = pd.read_csv('qm7_files/hof_qm7.txt', skiprows=0, delim_whitespace=True,  names = [\"input\",\"PBE0\",\"DFTB3\"])\n",
    "#print(data)\n",
    "#print(data.input)\n",
    "\n",
    "#Our X values will be the Coulomb matrices of the molecules,\n",
    "##therefore we need to have a list of paths to the xyz-files of the molecules\n",
    "inputfiles = []\n",
    "for i in range(n_molecules):\n",
    "    inputfile = \"%s\" %data.input[i] \n",
    "    inputfiles.append(inputfile)\n",
    "\n",
    "\n",
    "#Our Y values are the PBE0 energies\n",
    "Energy = np.zeros((n_molecules,1))\n",
    "for i in range(n_molecules):\n",
    "    Energy[i] = data.PBE0[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at some of these molecules.\n",
    "If you click an atom in the viewer, the last digit in the readout will tell you what element it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "import nglview as nv\n",
    "import MDAnalysis as mda\n",
    "\n",
    "print(\"The first entry is methane: \\nits atomization energy    = \", Energy[0], \"kcal/mol\")\n",
    "mda_traj = mda.Universe(inputfiles[0])\n",
    "mda_view = nv.show_mdanalysis(mda_traj)\n",
    "mda_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change k to inspect some molecules by yourself!\n",
    "k = 3485\n",
    "print(\"Atomization energy of entry \" + str(k) + \" =\", Energy[k], \"kcal/mol\")\n",
    "mda_traj = mda.Universe(inputfiles[k])\n",
    "mda_view = nv.show_mdanalysis(mda_traj)\n",
    "mda_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implementation of the Coulomb matrix descriptor\n",
    "\n",
    "The Coulomb matrix is defined as (Rupp et al., Phys. Rev. Lett., 108, 058301, 2012): \n",
    "\n",
    "$$\n",
    "M_{ij} = \n",
    "\\begin{cases}\n",
    "  0.5 Z_i^{2.4}  & \\mathrm{for} \\ i = j\\\\\n",
    "  \\frac{Z_i Z_j}{R_{ij}}  & \\mathrm{for} \\ i \\neq j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "where the diagonal elements describe the potential energy of isolated atoms and the off-diagonal elements represent the pair-wise repulsion between nuclei $i$ and $j$.\n",
    "\n",
    "We implement the Coulomb matrix descriptor based on https://github.com/pythonpanda/coulomb_matrix/blob/master/smiles_to_sparkcsv.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodicfunc(element):\n",
    "    \"\"\"\n",
    "    A function to output atomic number for each element in the periodic table\n",
    "    \"\"\"\n",
    "    f = open(\"pt.txt\")\n",
    "    atomicnum = [line.split()[1] for line in f if line.split()[0] == element]\n",
    "    f.close()\n",
    "    return int(atomicnum[0])\n",
    "\n",
    "def coulombmat(file,dim):\n",
    "    \"\"\"\n",
    "    This function takes in an xyz input file for a molecule, \n",
    "    and the number of atoms in the biggest possible molecule \n",
    "    to compute the corresponding Coulomb matrix \n",
    "    \"\"\"\n",
    "    xyzfile=open(file)\n",
    "    xyzheader = int(xyzfile.readline())\n",
    "    xyzfile.close()\n",
    "    i=0 ; j=0    \n",
    "    cij=np.zeros((dim,dim))\n",
    "    chargearray = np.zeros((xyzheader,1))\n",
    "    xyzmatrix = np.loadtxt(file,skiprows=2,usecols=[1,2,3])\n",
    "    atominfoarray = np.loadtxt(file,skiprows=2,dtype=str,usecols=[0])\n",
    "    chargearray = [periodicfunc(symbol)  for symbol in atominfoarray]\n",
    "    \n",
    "    for i in range(xyzheader):\n",
    "        for j in range(xyzheader):\n",
    "            if i == j:\n",
    "                cij[i,j]=0.5*chargearray[i]**2.4   # Diagonal term described by Potential energy of isolated atom\n",
    "            else:\n",
    "                dist = np.linalg.norm(xyzmatrix[i,:] - xyzmatrix[j,:])              \n",
    "                cij[i,j] = chargearray[i]*chargearray[j]/dist   #Pair-wise repulsion \n",
    "    return  cij\n",
    "\n",
    "def matsort(xyzfile,dim):\n",
    "    \"\"\"\n",
    "    Takes in a Coloumb matrix of (mxn) dimension and performs a rowwise sorting such that ||C(j,:)|| > ||C(j+1,:)||, J= 0,1,.......,(m-1)\n",
    "    Finally returns a vectorized (m*n,1) column matrix .\n",
    "    \"\"\"   \n",
    "    unsorted_mat = coulombmat(xyzfile,dim)\n",
    "    summation = np.array([sum(x**2) for x in unsorted_mat])\n",
    "    sorted_mat = unsorted_mat[np.argsort(summation)[::-1,],:]    \n",
    "    return sorted_mat.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Kernel Ridge Regression\n",
    "\n",
    "Kernel Ridge Regression is an approach to supervised learning in which we are inferring a relationship between input and output. In our case, the input will be a molecule represented by a vector $\\mathbf{X}$ and the output atomization energy $y$. We can view KRR as a basis set expansion since we write the relationship $\\mathbf{X} \\rightarrow y$ as:\n",
    "\n",
    "$$\n",
    "y(\\mathbf{X'}) = \\sum_i^N c_i K(\\mathbf{X'}, \\mathbf{X}_i)\n",
    "$$\n",
    "\n",
    "where our basis functions $K$ (the kernel functions) are centered on each training point and the $c_i$ are regression coefficients that can be obtained by minimizing a cost function. If we write the equation above in matrix form \n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{K} \\vec{c}\n",
    "$$\n",
    "the optimisation problem reads:\n",
    "\n",
    "$$\n",
    "\\quad \\text{arg min}_{\\vec{c}}\\left(\\frac{1}{2} ||\\mathbf{K} \\vec{c} - \\mathbf{y}^{\\mathrm{ref}} ||^2_2 + \\frac{\\alpha}{2} \\vec{c}^T \\mathbf{K} \\vec{c}\\right).\n",
    "$$\n",
    "\n",
    "This has the solution\n",
    "\n",
    "$$\n",
    "\\quad \\vec{c} = (\\mathbf{K} + \\alpha \\mathbb{1} )^{-1} \\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the regularization parameter which encourages smoother models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will use is the coefficient of determination as the metric of model performance or score:\n",
    "\n",
    "$$\n",
    "R^2 = \\left(1 - \\frac{u}{v}\\right),\n",
    "$$\n",
    "\n",
    "where $u$ is the residual sum of squares; $u = \\sum_i \\left( y_{i, \\text{True}} - y_{\\text{i, Pred}} \\right) $ and $v$ is the total sum of squares: $v = \\sum_i \\left( y_{i, \\text{True}} - \\overline{y}_{\\text{True}}\\right)$. $ \\overline{y}_{\\text{True}}$ is the overall mean of our observed energies. The best score possibe is 1 (since $u$ vanishes) and it can get below $0$. An example of a model with an $R^2$ of $0$ is a constant model that always predicts $\\overline{y}_{\\text{True}}$.\n",
    "\n",
    "Firstly, we will calculate the Coulomb matrix for each structure. Then, we will split our data into training and testing sets. This step could take a few minutes. During that continue reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define X (Coulomb matrices) and Y (energy)\n",
    "all_CM = []\n",
    "for i in inputfiles:\n",
    "    CM = matsort(i,23)\n",
    "    all_CM.append(CM)\n",
    "all_CM = np.array(all_CM)\n",
    "X = all_CM\n",
    "Y = Energy\n",
    "\n",
    "# 2. Split X and Y into training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42)\n",
    "\n",
    "# 3. Scale training and test set wrt. to the training set\n",
    "X_mean = np.mean(X_train)\n",
    "X_std  = np.std(X_train)\n",
    "X_train_scaled = (X_train - X_mean)/X_std\n",
    "X_test_scaled = (X_test - X_mean)/X_std\n",
    "X_scaled = np.array((X-X_mean)/X_std)\n",
    "\n",
    "# 4. Define 5-fold cross validation\n",
    "rkf=RepeatedKFold(n_splits=5, n_repeats=1,  random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared data and defined cross-validation. Have a look at the implementation of Kernel Ridge Regression using a linear kernel with 5-fold cross-validation. The optimal hyperparameter we are trying to find is the regularization parameter alpha for the linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LINEAR KERNEL\")\n",
    "print(\"Choice of alpha:\")\n",
    "all_scores={}\n",
    "all_var={}\n",
    "\n",
    "# Alpha in SciKit learn is the regularization parameter.\n",
    "alphas = [ 1e-4, 1e-3, 1e-2, 0.1, 1, 5]\n",
    "for alpha in alphas:\n",
    "    print(\"alpha = \",alpha)\n",
    "    score = []\n",
    "    for train_index, test_index in rkf.split(X_train,Y_train):\n",
    "        #We split the training set from before again in a training and validation set for each model.\n",
    "        ##ttrain=traintrain, val=validation\n",
    "        X_ttrain, X_val = X_train_scaled[train_index], X_train_scaled[test_index]\n",
    "        Y_ttrain, Y_val = Y_train[train_index], Y_train[test_index]\n",
    "        model = KernelRidge(kernel ='linear', alpha=alpha)\n",
    "        model.fit(X_ttrain,Y_ttrain)\n",
    "        Y_pred_val= model.predict(X_val)\n",
    "        #The coefficient R^2 is defined as (1 - u/v), where \n",
    "        ##u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and\n",
    "        ##v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). \n",
    "        score.append(model.score(X_val, Y_val))\n",
    "    print (\"Scores: \", score)\n",
    "    print (\"Average over cross validation: \", np.mean(score))\n",
    "    print (\"Variance: \", np.var(score))\n",
    "    print (\"----------------\\n\")\n",
    "    all_scores.update({alpha:np.mean(score)})\n",
    "    all_var.update({alpha:np.var(score)})\n",
    "print (\"All average scores: \", all_scores)\n",
    "print (\"All variances: \", all_var, \"\\n\")\n",
    "print (\"Best average score: \", max(all_scores, key=all_scores.get))\n",
    "print (\"Best variance: \", min(all_var, key=all_var.get))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Let us try out different kernel types and vary their parameters. Starting with a rough grid, we can check the performance at a fixed hyperparameter and then shrink the intervals to find optimal parameters.\n",
    "\n",
    "The Radial-basis function kernel (rbf) kernel is given by:\n",
    "$$\n",
    "K(\\mathbf{X}, (\\mathbf{Y}) = exp\\left( - \\gamma || \\mathbf{X} - \\mathbf{Y} ||^2 \\right)\n",
    "$$\n",
    "and the polynomial (poly) kernel by:\n",
    "$$\n",
    "K(\\mathbf{X}, (\\mathbf{Y}) = \\left( \\gamma \\langle \\mathbf{X} , \\mathbf{Y} \\rangle + 1 \\right)^{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These kernel types require more parameters, instead of doing a for-for loop let us import grid search from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "####################################\n",
    "#### YOUR TURN: try the 'rbf' and 'poly' kernel and vary the hyperparameters\n",
    "#define the model\n",
    "model = KernelRidge(kernel ='')\n",
    "    \n",
    "# define the hyperparameters (alpha, gamma) that should be investigated in the grid search\n",
    "# and choose their values. np.logspace(a, b, n) distributes n parameters logarithmically \n",
    "# between 10**a to 10**b.\n",
    "params = {'alpha' : np.logspace(-6, 4, 5),\n",
    "          'gamma': np.logspace(-6, 1, 5)\n",
    "         }\n",
    "####################################\n",
    "\n",
    "#define the grid search with cross validation included\n",
    "grid_search = GridSearchCV(estimator = model, #the estimator is your KRR model\n",
    "                        param_grid = params, #the parameters to vary\n",
    "                        scoring = 'r2', #R^2 scoring function\n",
    "                        cv = 3, #3-fold cross-validation\n",
    "                        verbose = 1, #more information in output\n",
    "                        n_jobs = -1, #number of cores used, -1 uses all available\n",
    "                        return_train_score=True)\n",
    "#fit the model\n",
    "grid_search.fit(X_train_scaled, Y_train)\n",
    "\n",
    "\n",
    "#We plot the performance of all hyperparameters\n",
    "pivot = pd.pivot_table(pd.DataFrame(grid_search.cv_results_),\n",
    "                       index='param_gamma', \n",
    "                       columns='param_alpha', \n",
    "                       values='mean_test_score')\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(pivot, cbar_kws={'label': 'mean $R^2$ score'})\n",
    "\n",
    "\n",
    "# extract best estimator\n",
    "print(\"Best estimator: \", grid_search.best_estimator_)\n",
    "# extract score of best estimator\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Test the best model on the test set (X_test_scaled, Y_test) and plot your predicted energies against the real ones in a scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare training and prediction speeds:\n",
    "import time\n",
    "\n",
    "####################################\n",
    "####YOUR TURN: try your best model, here is a bad one:\n",
    "#define the model\n",
    "model = KernelRidge(kernel = \"sigmoid\", alpha = 0.4, gamma = 0.0001)\n",
    "\n",
    "####################################\n",
    "\n",
    "#Fit the model and predict energies\n",
    "start = time.time()\n",
    "model.fit(X_train_scaled,Y_train)\n",
    "end = time.time()\n",
    "print(\"Wall-clock time for training:\", end-start)\n",
    "\n",
    "start = time.time()\n",
    "Y_prediction= model.predict(X_test_scaled)\n",
    "end = time.time()\n",
    "print(\"Wall-clock time for predicting:\", end-start)\n",
    "\n",
    "print(\"MAE: \", np.mean(np.abs(Y_prediction-Y_test)), \"kcal/mol\")\n",
    "print(\"MSE: \", np.mean(np.abs(Y_prediction-Y_test)**2))\n",
    "print(\"Score: \", model.score(X_test_scaled, Y_test))\n",
    "\n",
    "#Plot result\n",
    "plt.figure()\n",
    "plt.scatter(Y_test,Y_prediction)\n",
    "a = np.linspace(-2500,0)\n",
    "plt.plot(a,a,color=\"r\")\n",
    "plt.xlabel(\"E(test) [kcal/mol]\")\n",
    "plt.ylabel(\"E(prediction) [kcal/mol]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Support Vector Regression\n",
    "\n",
    "In support vector regression, samples lying outside the $\\epsilon$-tube are penalized by their distance to the margin boundary: $\\zeta_i , \\zeta_i^* \\geq 0$, depending on whether they lie above or below the tube. The penalty has the form $C \\sum_{i=1}^n (\\zeta_i + \\zeta_i^*)$, thus $C$ acts as an inverse regularization parameter.\n",
    "\n",
    "A grid search for support vector regression takes on a similar shape, but with the tubewidth $\\epsilon$ and the penalizations strength $C$. In contrast to KRR, fitting a SVR model can not be done in closed-form and training is typically slower. On the other hand, since SVR learns a sparse model (in contrast to KRR) one expects the prediction to be faster (for large models). The additional arithmetic operations required in KRR may however be compensated by computational details in how the kernel functions are computed.\n",
    "\n",
    "**Task 3:** Let us try out different kernel types and vary their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import grid search from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "####################################\n",
    "####YOUR TURN: try the 'rbf' and 'poly' kernel and vary the hyperparameters\n",
    "#define the model\n",
    "model = SVR(kernel ='')\n",
    "\n",
    "#define the hyperparameters (C, epsilon) that should be investigated in the grid search\n",
    "#and choose their values \n",
    "#default: gamma='scale'\n",
    "params = {'C' : np.logspace(-1, 4, 5),\n",
    "          'epsilon': np.logspace(-1, 2, 5)\n",
    "         }\n",
    "####################################\n",
    "\n",
    "\n",
    "#define the grid search with cross validation included\n",
    "grid_search = GridSearchCV(estimator = model, #the estimator is your KRR model\n",
    "                        param_grid = params, #the parameters to vary\n",
    "                        scoring = 'r2', #R^2 scoring function\n",
    "                        cv = 3, #3-fold cross-validation\n",
    "                        verbose = 1, #more information in output\n",
    "                        n_jobs = -1, #number of cores used, -1 uses all available\n",
    "                        return_train_score=True)\n",
    "#fit the model\n",
    "grid_search.fit(X_train_scaled, Y_train.ravel())\n",
    "\n",
    "\n",
    "#We plot the performance of all hyperparameters\n",
    "pivot = pd.pivot_table(pd.DataFrame(grid_search.cv_results_),\n",
    "                       index='param_C', \n",
    "                       columns='param_epsilon', \n",
    "                       values='mean_test_score')\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "sns.heatmap(pivot, cbar_kws={'label': 'mean $R^2$ score'})\n",
    "\n",
    "# extract best estimator\n",
    "print(\"Best estimator: \", grid_search.best_estimator_)\n",
    "# extract score of best estimator\n",
    "print(\"Best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the rbf kernel we found:<br> \n",
    "Best estimator:  SVR(C=10000.0, epsilon=0.6812920690579614)<br> \n",
    "With score:  0.9900406583028621\n",
    "\n",
    "With the poly kernel we found:<br>\n",
    "Best estimator:  SVR(C=100000.0, epsilon=35.93813663804626, kernel='poly')<br>\n",
    "With score:  0.9378256191280022\n",
    "\n",
    "Have you managed to find a better model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Test the kernel on the test set (X_test_scaled, Y_test) and plot your predicted energies against the real ones in a scatter plot. How does the wall time for training compare to KRR? How about prediction time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma='scale' #default\n",
    "\n",
    "####################################\n",
    "#### YOUR TURN: try your best model, here is a bad one:\n",
    "#define the model \n",
    "model = SVR(kernel ='sigmoid', epsilon=0.111, C=1214, gamma=gamma)\n",
    "####################################\n",
    "\n",
    "#Fit the model and predict energies\n",
    "start = time.time()\n",
    "model.fit(X_train_scaled,Y_train)\n",
    "end = time.time()\n",
    "print(\"Wall-clock time for training:\", end-start)\n",
    "\n",
    "start = time.time()\n",
    "Y_prediction= model.predict(X_test_scaled)\n",
    "end = time.time()\n",
    "print(\"Wall-clock time for predicting:\", end-start)\n",
    "\n",
    "train_size = len(Y_train)\n",
    "sv_ratio = model.support_.shape[0] / train_size\n",
    "print(\"Support vector ratio: %.3f\" % sv_ratio)\n",
    "\n",
    "print(\"MAE: \", np.mean(np.abs(Y_prediction-Y_test)), \"kcal/mol\")\n",
    "print(\"MSE: \", np.mean(np.abs(Y_prediction-Y_test)**2))\n",
    "print(\"Score: \", model.score(X_test_scaled, Y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(Y_test,Y_prediction)\n",
    "a = np.linspace(-2500,0)\n",
    "plt.plot(a,a,color=\"r\")\n",
    "plt.xlabel(\"E(test) [kcal/mol]\")\n",
    "plt.ylabel(\"E(prediction [kcal/mol])\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
